from sentence_transformers import SentenceTransformer
import faiss
from llama_cpp import Llama

from config import config
from utils import get_logger, load_documents

def make_prompt(context, query):
    return (
        "<|user|>\n"
        "ç¤¾å†…ãƒ«ãƒ¼ãƒ«ã«è¼‰ã£ã¦ã„ã‚‹ã“ã¨ã®ã¿ã«åŸºã¥ã„ã¦ç­”ãˆã¦ãã ã•ã„ï¼"
        f"ç¤¾å†…ãƒ«ãƒ¼ãƒ«ï¼š{context}\n\n"
        f"è³ªå•ï¼š{query}\n"
        "<|end|>\n"
        "<|assistant|>\n"
    )

logger = get_logger()
logger.info("ğŸ“¦ Loading Index and Document...")
index = faiss.read_index(config["INDEX_PATH"])


cat_documents = load_documents(json_path=config["DOC_PATH"])

embedder = SentenceTransformer(config["EMBED_MODEL"])

logger.info("ğŸ“¦ Crating LLM Model...")
llm = Llama(
    model_path=config["MODEL_PATH"],
    n_ctx=2048,
    n_threads=4,
)

query = "ç¤¾å†…è¦å‰‡ã«é–¢ã—ã¦ï¼Œæœã®æŒ¨æ‹¶ã®ãƒ«ãƒ¼ãƒ«ã‚’æ•™ãˆã¦ãã ã•ã„"
# query = "æœã®æŒ¨æ‹¶ã£ã¦ã©ã“ã¾ã§ä¸å¯§ã«ã—ãŸã»ã†ãŒã„ã„ã§ã™ã‹ï¼Ÿ"
# query = "æ°´ã‚’é£²ã¿ã«è¡Œã£ã¦ãã¦ã‚‚ã„ã„ã§ã™ã‹ï¼Ÿ"
# query = "ä»Šæ—¥ã€å®šæ™‚ã§å¸°ã£ã¦ã„ã„ã§ã™ã‹ï¼Ÿ"
query_vec = embedder.encode([query])
distance, indices = index.search(query_vec, config["TOP_K"])

context_chunks = [cat_documents[i] for i in indices[0]]
context_text = "\n\n".join(context_chunks)

logger.info("[RAG ãªã—] æ¨è«–ä¸­...")
prompt_wo_RAG = make_prompt("", query)
res_wo_RAG = llm(prompt_wo_RAG, max_tokens=256, stop=["<|end|>", "<|user|>", "<|assistant|>"])
answer_wo_RAG = res_wo_RAG["choices"][0]["text"].strip()

logger.info("[RAG ã‚ã‚Š] æ¨è«–ä¸­...")
prompt_w_RAG = make_prompt(context_text, query)
res_w_RAG = llm(prompt_w_RAG, max_tokens=256, stop=["<|end|>", "<|user|>", "<|assistant|>"])
answer_w_RAG = res_w_RAG["choices"][0]["text"].strip()

logger.info("[RAG ãªã—] answer")
print(answer_wo_RAG)
print()
logger.info("[RAG ã‚ã‚Š] answer")
print(answer_w_RAG)